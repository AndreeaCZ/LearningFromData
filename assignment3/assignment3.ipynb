{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''TODO: add high-level description of this Python script'''\n",
    "\n",
    "import random as python_random\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from keras.src.models import Sequential\n",
    "from keras.src.layers import Dense, Embedding, LSTM\n",
    "from keras.src.initializers import Constant\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.src.optimizers import SGD\n",
    "from keras.src.layers import TextVectorization\n",
    "import keras as kr\n",
    "import tensorflow as tf\n",
    "# Make reproducible as much as possible\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "python_random.seed(1234)\n",
    "\n",
    "\n",
    "def create_arg_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-i\", \"--train_file\", default='train.txt', type=str,\n",
    "                        help=\"Input file to learn from (default train.txt)\")\n",
    "    parser.add_argument(\"-d\", \"--dev_file\", type=str, default='dev.txt',\n",
    "                        help=\"Separate dev set to read in (default dev.txt)\")\n",
    "    parser.add_argument(\"-t\", \"--test_file\", type=str,\n",
    "                        help=\"If added, use trained model to predict on test set\")\n",
    "    parser.add_argument(\"-e\", \"--embeddings\", default='glove_reviews.json', type=str,\n",
    "                        help=\"Embedding file we are using (default glove_reviews.json)\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def read_corpus(corpus_file):\n",
    "    '''Read in review data set and returns docs and labels'''\n",
    "    documents = []\n",
    "    labels = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip()\n",
    "            documents.append(\" \".join(tokens.split()[3:]).strip())\n",
    "            # 6-class problem: books, camera, dvd, health, music, software\n",
    "            labels.append(tokens.split()[0])\n",
    "    return documents, labels\n",
    "\n",
    "\n",
    "def read_embeddings(embeddings_file):\n",
    "    '''Read in word embeddings from file and save as numpy array'''\n",
    "    embeddings = json.load(open(embeddings_file, 'r'))\n",
    "    return {word: np.array(embeddings[word]) for word in embeddings}\n",
    "\n",
    "\n",
    "def get_emb_matrix(voc, emb):\n",
    "    '''Get embedding matrix given vocab and the embeddings'''\n",
    "    num_tokens = len(voc) + 2\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    # Bit hacky, get embedding dimension from the word \"the\"\n",
    "    embedding_dim = len(emb[\"the\"])\n",
    "    # Prepare embedding matrix to the correct size\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = emb.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    # Final matrix with pretrained embeddings that we can feed to embedding layer\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def create_model(Y_train, emb_matrix):\n",
    "    '''Create the Keras model to use'''\n",
    "    # Define settings, you might want to create cmd line args for them\n",
    "    learning_rate = 0.01\n",
    "    loss_function = 'categorical_crossentropy'\n",
    "    optim = SGD(learning_rate=learning_rate)\n",
    "    # Take embedding dim and size from emb_matrix\n",
    "    embedding_dim = len(emb_matrix[0])\n",
    "    num_tokens = len(emb_matrix)\n",
    "    num_labels = len(set(Y_train))\n",
    "    # Now build the model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_tokens, embedding_dim, embeddings_initializer=Constant(emb_matrix),trainable=False))\n",
    "    # Here you should add LSTM layers (and potentially dropout)\n",
    "    raise NotImplementedError(\"Add LSTM layer(s) here\")\n",
    "    # Ultimately, end with dense layer with softmax\n",
    "    model.add(Dense(input_dim=embedding_dim, units=num_labels, activation=\"softmax\"))\n",
    "    # Compile model using our settings, check for accuracy\n",
    "    model.compile(loss=loss_function, optimizer=optim, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train, Y_train, X_dev, Y_dev):\n",
    "    '''Train the model here. Note the different settings you can experiment with!'''\n",
    "    # Potentially change these to cmd line args again\n",
    "    # And yes, don't be afraid to experiment!\n",
    "    verbose = 1\n",
    "    batch_size = 16\n",
    "    epochs = 50\n",
    "    # Early stopping: stop training when there are three consecutive epochs without improving\n",
    "    # It's also possible to monitor the training loss with monitor=\"loss\"\n",
    "    # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    callback = kr.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    # Finally fit the model to our data\n",
    "    model.fit(X_train, Y_train, verbose=verbose, epochs=epochs, callbacks=[callback], batch_size=batch_size, validation_data=(X_dev, Y_dev))\n",
    "    # Print final accuracy for the model (clearer overview)\n",
    "    test_set_predict(model, X_dev, Y_dev, \"dev\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_set_predict(model, X_test, Y_test, ident):\n",
    "    '''Do predictions and measure accuracy on our own test set (that we split off train)'''\n",
    "    # Get predictions using the trained model\n",
    "    Y_pred = model.predict(X_test)\n",
    "    # Finally, convert to numerical labels to get scores with sklearn\n",
    "    Y_pred = np.argmax(Y_pred, axis=1)\n",
    "    # If you have gold data, you can calculate accuracy\n",
    "    Y_test = np.argmax(Y_test, axis=1)\n",
    "    print('Accuracy on own {1} set: {0}'.format(round(accuracy_score(Y_test, Y_pred), 3), ident))\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''Main function to train and test neural network given cmd line arguments'''\n",
    "    args = create_arg_parser()\n",
    "\n",
    "    # Read in the data and embeddings\n",
    "    X_train, Y_train = read_corpus(args.train_file)\n",
    "    X_dev, Y_dev = read_corpus(args.dev_file)\n",
    "    embeddings = read_embeddings(args.embeddings)\n",
    "\n",
    "    # Transform words to indices using a vectorizer\n",
    "    vectorizer = TextVectorization(standardize=None, output_sequence_length=50)\n",
    "    # Use train and dev to create vocab - could also do just train\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(X_train + X_dev)\n",
    "    vectorizer.adapt(text_ds)\n",
    "    # Dictionary mapping words to idx\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    emb_matrix = get_emb_matrix(voc, embeddings)\n",
    "\n",
    "    # Transform string labels to one-hot encodings\n",
    "    encoder = LabelBinarizer()\n",
    "    Y_train_bin = encoder.fit_transform(Y_train)  # Use encoder.classes_ to find mapping back\n",
    "    Y_dev_bin = encoder.fit_transform(Y_dev)\n",
    "\n",
    "    # Create model\n",
    "    model = create_model(Y_train, emb_matrix)\n",
    "\n",
    "    # Transform input to vectorized input\n",
    "    X_train_vect = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
    "    X_dev_vect = vectorizer(np.array([[s] for s in X_dev])).numpy()\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(model, X_train_vect, Y_train_bin, X_dev_vect, Y_dev_bin)\n",
    "\n",
    "    # Do predictions on specified test set\n",
    "    if args.test_file:\n",
    "        # Read in test set and vectorize\n",
    "        X_test, Y_test = read_corpus(args.test_file)\n",
    "        Y_test_bin = encoder.fit_transform(Y_test)\n",
    "        X_test_vect = vectorizer(np.array([[s] for s in X_test])).numpy()\n",
    "        # Finally do the predictions\n",
    "        test_set_predict(model, X_test_vect, Y_test_bin, \"test\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "480703bc041626ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
